# -*- coding: utf-8 -*-
"""mazourik_assignment3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o5_ZEN-ZvtyJPq6-fAIxeFjr3-6q7qjn
"""

import tensorflow as tf
import numpy as np
with open('monte_cristo.txt', 'r') as f:
  text=''
  for i in f:
    text+=i.lower()
  print(f)
  print(type(f))
      #f=f.lower()

unique_chars={}
int_char_dic={}
for i in text:
  if i not in unique_chars:
    unique_chars[i]=0
  unique_chars[i]+=1

print('Total number of characters:', sum(unique_chars.values()))
print('Number of Unique Characters:', len(unique_chars))
print('Deleting the rare cases...')
acceptable_rarity=sum(unique_chars.values())/10e3

for i in unique_chars.copy():
  if unique_chars[i]<acceptable_rarity:
    #print('Deleting Rare Character:', i)
    del unique_chars[i]

print('New Number of Unique Characters:', len(unique_chars))
for i in range(len(unique_chars)):
  int_char_dic[list(unique_chars.keys())[i]]=i

parsed_text=[i for i in text if i in list(unique_chars.keys())]
one_text=[int_char_dic[i] for i in parsed_text]
one_hot_text=[]
#Tensorflow one hot does not take strings as input
for i in range(len(one_text)):
  one_hot_text.append(np.zeros(len(unique_chars)))
  one_hot_text[i][one_text[i]]=1

#Everything for step #2 has been completed.

#Part 3 should be complete
def generate_batches(text , batch_size , sequence_length):
  #// is integer division to get the length of the blocks
  #The structure in the end is Texts => Blocks (Batches) => Sequences => One Hot Encoding of Chars. So really the final shape should be k, 16, 256, 49?
  block_length = len(text) // batch_size
  batches = []

  for i in range(0 , block_length , sequence_length):
    batch = []

    for j in range(batch_size):
      start = j*block_length + i
      end = min(start + sequence_length, j*block_length + block_length) 
      #batch=something, that something should be a list of sequences that is found within the block
      batch.append(text[start:end])
    batches.append(np.array(batch, dtype=int)) 

  del batches[-1]
  return np.array(batches)
#dataset=generate_batches(one_hot_text, 16, 256)

dataset=generate_batches(one_hot_text, 16, 256)
X_data=dataset[:,:,:-1]
Y_data=dataset[:,:,1:]
tf.reset_default_graph()
# Model parameters
hidden_units = 256 
# Number of recurrent units
# Training procedure parameters
learning_rate = 1e-2
n_epochs = 5
batch_size = 16
unique_chars=dataset.shape[-1]

# Model definition
#Neural Network needs 1 MultiRNN cells with 2 LSTM with 256 units then through fully connected layer and softmax with k units. Sample from it.
X = tf.placeholder(shape=[batch_size, 255, 49], dtype=tf.float64) 
Y = tf.placeholder(shape=[batch_size, 255, 49], dtype=tf.float64)

length = tf.placeholder(shape=[None], dtype=tf.int64) 

state=tf.placeholder(shape=[None], dtype=tf.float64)

# Defining the Neural Network
#Two starting state, h and c. for both

num_units = [256, 256]
cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in num_units]

cell_1=tf.nn.rnn_cell.MultiRNNCell(cells)

init_state=cell_1.zero_state(batch_size, dtype=tf.float64)
rnn_outputs, final_state = tf.nn.dynamic_rnn(cell_1, X, sequence_length=length, initial_state=init_state)


#Fully Connected Layer 
rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, hidden_units])

Wout = tf.Variable(tf.truncated_normal(shape=(hidden_units, unique_chars), stddev=0.1, dtype=tf.float64))
bout = tf.Variable(tf.zeros(shape=[unique_chars], dtype=tf.float64))

Z = tf.matmul(rnn_outputs_flat, Wout) + bout

Y_flat = tf.reshape(Y, [-1, unique_chars])

#Softmax Optimisation
loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_flat, logits=Z) 
optimizer = tf.train.AdamOptimizer(learning_rate)
train = optimizer.minimize(loss)
session = tf.Session()
session.run(tf.global_variables_initializer())

#Train

lengths= [256] * 16

[256]*16

batch_acc=[]
n_epochs=5
for e in range(1, n_epochs + 1):
  print(f'epoch {e}')
  for i in range(dataset.shape[0]):
    present_state=session.run([final_state],
                              {X: X_data[i], 
                                length: lengths,
                                })
    feed = {X: X_data[i], 
            Y: Y_data[i], 
            init_state: present_state, 
            length: lengths,
            } 
    l, state_1 = session.run([loss, train], feed)
    print('Batch: {0}. Loss: {1}.'.format(i,np.mean(l)))
    batch_acc.append(np.mean(l))
  #Overwrite the state

  print('Epoch: {0}. Loss: {1}.'.format(e, np.mean(l)))

# Shows first task and corresponding prediction

#Creating new strings
sentences=[]
for sentence in range(20):
  init_string='i '

  inv_dic = {x: y for y, x in int_char_dic.items()}

  one_hot_text=[]

  #Tensorflow one hot does not take strings as input


  for i in range(256-len(init_string)):
    one_hot_text= np.zeros([16,255,49])
    #Tensorflow one hot does not take strings as input
    
    for j,character in enumerate(init_string[::-1]):
      index = int_char_dic[character]
      #initstring[]
      j+=1
      one_hot_text[-1][-j][index]=1

    present_state=session.run([final_state],
                                  {X: one_hot_text,
                                  length: lengths,
                                   })
    feed = {X: one_hot_text,  
            init_state: present_state, 
            length: lengths}

    Z_new=session.run([Z], feed)
  
    softmax = np.exp(Z_new[-1][-1])
    softmax = softmax/ np.sum(softmax)
    draw = np.random.choice(list(range(49)), p=softmax)
    init_string+=inv_dic[draw]
  

  print(init_string)
  sentences.append(init_string)

import matplotlib.pyplot as plt
plt.plot(batch_acc)
plt.ylabel('Loss')
plt.xlabel('Batches')
plt.show();